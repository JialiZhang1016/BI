## Thinking1 XGBoost与GBDT的区别是什么？
**XGBoost(Extreme Gradient Boosting)**  
**GBDT(Gradient Boosting Decision Tree)**
1. XGBoost是陈天奇等人开发的一个开源机器学习项目，高效地实现了GBDT算法并进行了算法和工程上的许多改进。
2. 在使用CART作为基分类器时，XGBoost显式地加入了正则项来控制模型的复杂度，有利于防止过拟合，从而提高模型的泛化能力。
3. GBDT在模型训练时只使用了代价函数的一阶导数信息，XGBoost对代价函数进行二阶泰勒展开，可以同时使用一阶和二阶导数。
4. 传统的GBDT采用CART作为基分类器，XGBoost支持多种类型的基分类器，比如线性分类器。
5. 传统的GBDT在每轮迭代时使用全部的数据，XGBoost则采用了与随机森林相似的策略，支持对数据进行采样，支持列抽样，不仅能降低过拟合，还能减少计算，这也是xgboost异于传统gbdt的一个特性。
6. 传统的GBDT没有设计对缺失值进行处理，XGBoost可以自动学习出它的分裂方向。XGBoost对于确实值能预先学习一个默认的分裂方向。
7. Shrinkage（缩减），相当于学习速率（xgboost中的eta）。xgboost在进行完一次迭代后，会将叶子节点的权重乘上该系数，主要是为了削弱每棵树的影响，让后面有更大的学习空间。实际应用中，一般把eta设置得小一点，然后迭代次数设置得大一点。（补充：传统GBDT的实现也有学习速率）

参考：  
[Adaboost, GBDT 与 XGBoost 的区别](https://zhuanlan.zhihu.com/p/42740654)  
[GBDT、XGBoost、LightGBM的区别和联系](https://www.jianshu.com/p/765efe2b951a)

## Thinking2 举一个你之前做过的预测例子
我之前基于C5.0决策树做过银行个人信用风险评估模型（使用R）。
这个项目基于银联商务发布的 11017 条个人信用贷款数据，使用C5.0算法建立决策树评价模型。
并在原模型的基础上分别利用 AdaBoost 算法和惩罚因子对其进行优化。
通过比较发现，引入惩罚因子能够有效地将模型对逾期用户的误判率从93.2%降低至31.98 %。
虽然总体预测正确率有所下降，但从经济效益方面看，能够切实地帮助银行控制违约风险。

## Thinking3 请你思考，在你的工作中，需要构建哪些特征，这些特征都包括哪些维度
比如在[员工离职预测](https://www.kaggle.com/c/bi-attrition-prediction)的项目中,可以从员工、公司、行业三个方向构建特征。
- 员工个人特征方面可以有年龄，性别，公司跟家庭住址的距离，受教育程度，专业领域，员工对于工作环境的满意程度等；  
- 公司特征方面可以有员工曾经工作过的公司数，标准工时，股票期权水平，在目前公司工作年数，在目前工作职责的工作年数，跟目前的管理者共事年数等。
